{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"testingTextRankPreprocessing.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOdcdJagBI+UMLWsmnzocWr"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"V0rJqGFG3bUh","colab_type":"code","colab":{}},"source":["# %cd data\n","# !pwd\n","# !gunzip GoogleNews-vectors-negative300.bin\n","# %cd .."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qTDbHNDxzWNR","colab_type":"code","colab":{}},"source":["#INPUTS\n","\n","#repoPath should start with the path \n","#'/content/gdrive/My Drive/' and go to the folder where the git repo is, and should end with the name of the repo:\n","repoPath = '/content/gdrive/My Drive/TestingGitCloneIntoGoogleDrive/findingKeyphraseUsingTextRank'\n","userEmail = 'sanmesh1@gmail.com'\n","userName = 'sanmesh1'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dT11jlEGye8a","colab_type":"code","outputId":"ef4c7f5d-afb3-4e09-d89e-a61a4b72624b","executionInfo":{"status":"ok","timestamp":1591217560051,"user_tz":240,"elapsed":463,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BxNLdrvrzAFu","colab_type":"code","colab":{}},"source":["! pip install ipdb -q\n","import ipdb \n","\n","#place the line of code below from whichever line you want to start debugging\n","#type n for step over and s for step into\n","# ipdb.set_trace()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WROFG7cizCP3","colab_type":"code","colab":{}},"source":["import os\n","os.chdir(repoPath)\n","#%cd gdrive/My Drive/project_folder/TextSimilarityUsingWord2Vec"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KKQtAAu71T68","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":173},"outputId":"0e4f89a1-cfc5-4576-c791-d7c9a9c965cd","executionInfo":{"status":"ok","timestamp":1591215575111,"user_tz":240,"elapsed":1024,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}}},"source":["import spacy\n","nlp = spacy.load('en_core_web_sm')\n","content = '''\n","The Wandering Earth, described as China’s first big-budget science fiction thriller, quietly made it onto screens at AMC theaters in North America this weekend, and it shows a new side of Chinese filmmaking — one focused toward futuristic spectacles rather than China’s traditionally grand, massive historical epics. At the same time, The Wandering Earth feels like a throwback to a few familiar eras of American filmmaking. While the film’s cast, setting, and tone are all Chinese, longtime science fiction fans are going to see a lot on the screen that reminds them of other movies, for better or worse.\n","'''\n","doc = nlp(content)\n","for sents in doc.sents:\n","    print(sents.text)\n","    print()"],"execution_count":6,"outputs":[{"output_type":"stream","text":["\n","The Wandering Earth, described as China’s first big-budget science fiction thriller, quietly made it onto screens at AMC theaters in North America this weekend, and it shows a new side of Chinese filmmaking — one focused toward futuristic spectacles rather than China’s traditionally grand, massive historical epics.\n","\n","At the same time, The Wandering Earth feels like a throwback to a few familiar eras of American filmmaking.\n","\n","While the film’s cast, setting, and tone are all Chinese, longtime science fiction fans are going to see a lot on the screen that reminds them of other movies, for better or worse.\n","\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"U0RfQwBR12S1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"b813273a-8892-4c85-cc6f-028218ee8d2c","executionInfo":{"status":"ok","timestamp":1591215753691,"user_tz":240,"elapsed":429,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}}},"source":["candidate_pos = ['NOUN', 'PROPN', 'VERB']\n","sentences = []\n","\n","for sent in doc.sents:\n","  selected_words = []\n","  for token in sent:\n","    if token.pos_ in candidate_pos and token.is_stop is False:\n","      selected_words.append(token)\n","  sentences.append(selected_words)\n","print(sentences)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["[[Wandering, Earth, described, China, budget, science, fiction, thriller, screens, AMC, theaters, North, America, weekend, shows, filmmaking, focused, spectacles, China, epics], [time, Wandering, Earth, feels, throwback, eras, filmmaking], [film, cast, setting, tone, Chinese, science, fiction, fans, going, lot, screen, reminds, movies]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5-2lqDS31lFS","colab_type":"code","colab":{}},"source":["\n","from collections import OrderedDict\n","import numpy as np\n","import spacy\n","from spacy.lang.en.stop_words import STOP_WORDS\n","\n","nlp = spacy.load('en_core_web_sm')\n","\n","class TextRank4Keyword():\n","    \"\"\"Extract keywords from text\"\"\"\n","    \n","    def __init__(self):\n","        self.d = 0.85 # damping coefficient, usually is .85\n","        self.min_diff = 1e-5 # convergence threshold\n","        self.steps = 10 # iteration steps\n","        self.node_weight = None # save keywords and its weight\n","\n","    \n","    def set_stopwords(self, stopwords):  \n","        \"\"\"Set stop words\"\"\"\n","        for word in STOP_WORDS.union(set(stopwords)):\n","            lexeme = nlp.vocab[word]\n","            lexeme.is_stop = True\n","    \n","    def sentence_segment(self, doc, candidate_pos, lower):\n","        \"\"\"Store those words only in cadidate_pos\"\"\"\n","        sentences = []\n","        for sent in doc.sents:\n","            selected_words = []\n","            for token in sent:\n","                # Store words only with cadidate POS tag\n","                if token.pos_ in candidate_pos and token.is_stop is False:\n","                    if lower is True:\n","                        selected_words.append(token.text.lower())\n","                    else:\n","                        selected_words.append(token.text)\n","            sentences.append(selected_words)\n","        return sentences\n","        \n","    def get_vocab(self, sentences):\n","        \"\"\"Get all tokens\"\"\"\n","        vocab = OrderedDict()\n","        i = 0\n","        for sentence in sentences:\n","            for word in sentence:\n","                if word not in vocab:\n","                    vocab[word] = i\n","                    i += 1\n","        return vocab\n","    \n","    def get_token_pairs(self, window_size, sentences):\n","        \"\"\"Build token_pairs from windows in sentences\"\"\"\n","        token_pairs = list()\n","        for sentence in sentences:\n","            for i, word in enumerate(sentence):\n","                for j in range(i+1, i+window_size):\n","                    if j >= len(sentence):\n","                        break\n","                    pair = (word, sentence[j])\n","                    if pair not in token_pairs:\n","                        token_pairs.append(pair)\n","        return token_pairs\n","        \n","    def symmetrize(self, a):\n","        return a + a.T - np.diag(a.diagonal())\n","    \n","    def get_matrix(self, vocab, token_pairs):\n","        \"\"\"Get normalized matrix\"\"\"\n","        # Build matrix\n","        vocab_size = len(vocab)\n","        g = np.zeros((vocab_size, vocab_size), dtype='float')\n","        for word1, word2 in token_pairs:\n","            i, j = vocab[word1], vocab[word2]\n","            g[i][j] = 1\n","            \n","        # Get Symmeric matrix\n","        g = self.symmetrize(g)\n","        \n","        # Normalize matrix by column\n","        norm = np.sum(g, axis=0)\n","        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n","        \n","        return g_norm\n","\n","    \n","    def get_keywords(self, number=10):\n","        \"\"\"Print top number keywords\"\"\"\n","        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n","        for i, (key, value) in enumerate(node_weight.items()):\n","            print(key + ' - ' + str(value))\n","            if i > number:\n","                break\n","        \n","        \n","    def analyze(self, text, \n","                candidate_pos=['NOUN', 'PROPN'], \n","                window_size=4, lower=False, stopwords=list()):\n","        \"\"\"Main function to analyze text\"\"\"\n","        \n","        # Set stop words\n","        self.set_stopwords(stopwords)\n","        \n","        # Pare text by spaCy\n","        doc = nlp(text)\n","        \n","        # Filter sentences\n","        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n","        \n","        # Build vocabulary\n","        vocab = self.get_vocab(sentences)\n","        \n","        # Get token_pairs from windows\n","        token_pairs = self.get_token_pairs(window_size, sentences)\n","        \n","        # Get normalized matrix\n","        g = self.get_matrix(vocab, token_pairs)\n","        \n","        # Initionlization for weight(pagerank value)\n","        pr = np.array([1] * len(vocab))\n","        \n","        # Iteration\n","        previous_pr = 0\n","        for epoch in range(self.steps):\n","            pr = (1-self.d) + self.d * np.dot(g, pr)\n","            if abs(previous_pr - sum(pr))  < self.min_diff:\n","                break\n","            else:\n","                previous_pr = sum(pr)\n","\n","        # Get weight for each node\n","        node_weight = dict()\n","        for word, index in vocab.items():\n","            node_weight[word] = pr[index]\n","        \n","        self.node_weight = node_weight"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CjGXR4Tm2vso","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"outputId":"6f5943f5-3138-440a-fe3a-cc8e5966824e","executionInfo":{"status":"ok","timestamp":1591217580968,"user_tz":240,"elapsed":788,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}}},"source":["text = '''\n","The Wandering Earth, described as China’s first big-budget science fiction thriller, quietly made it onto screens at AMC theaters in North America this weekend, and it shows a new side of Chinese filmmaking — one focused toward futuristic spectacles rather than China’s traditionally grand, massive historical epics. At the same time, The Wandering Earth feels like a throwback to a few familiar eras of American filmmaking. While the film’s cast, setting, and tone are all Chinese, longtime science fiction fans are going to see a lot on the screen that reminds them of other movies, for better or worse.\n","'''\n","tr4w = TextRank4Keyword()\n","tr4w.analyze(text, candidate_pos = ['NOUN', 'PROPN'], window_size=4, lower=False)\n","tr4w.get_keywords(10)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["science - 1.7276378287292111\n","fiction - 1.7123791481736559\n","filmmaking - 1.4388798751402918\n","China - 1.4172218029410266\n","Earth - 1.3088154732297723\n","Chinese - 1.0971675275482093\n","fans - 1.0971675275482093\n","Wandering - 1.0071059904601571\n","weekend - 1.002449354657688\n","America - 0.9976329264870932\n","budget - 0.9769693829073562\n","North - 0.9711240881032547\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rw_YCIzI2vg1","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2XDGewEK1Tk1","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PC6_hu67zEs2","colab_type":"code","colab":{}},"source":["from time import time\n","start_nb = time()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bNWPXq1nz5k6","colab_type":"code","outputId":"3c099075-f22c-4fec-870e-c20b8b34c835","executionInfo":{"status":"ok","timestamp":1591166986289,"user_tz":240,"elapsed":2091,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# Import and download stopwords from NLTK.\n","from nltk.corpus import stopwords\n","from nltk import download\n","download('stopwords')  # Download stopwords list.\n","\n","# Remove stopwords.\n","stop_words = stopwords.words('english')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qPDjGOSXz9MH","colab_type":"code","outputId":"94cd27a4-9ed1-4396-adb2-37e11a3f91ef","executionInfo":{"status":"ok","timestamp":1591167060544,"user_tz":240,"elapsed":72123,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["start = time()\n","import os\n","from gensim.models import Word2Vec\n","\n","%cd data/\n","!ls\n","import os.path\n","from os import path\n","if path.exists(\"GoogleNews-vectors-negative300.bin\") == False:\n","  !wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n","  !gunzip GoogleNews-vectors-negative300.bin\n","else:\n","  print(\"datasetAlreadyExists\")\n","%cd ..\n","\n","from gensim.models.keyedvectors import KeyedVectors\n","model_path = './data/GoogleNews-vectors-negative300.bin'\n","model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n","\n","print('Cell took %.2f seconds to run.' % (time() - start))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/TestingGitCloneIntoGoogleDrive/textSimilarityUsingWordMoversDistance/data\n","GoogleNews-vectors-negative300.bin\n","datasetAlreadyExists\n","/content/gdrive/My Drive/TestingGitCloneIntoGoogleDrive/textSimilarityUsingWordMoversDistance\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"},{"output_type":"stream","text":["Cell took 71.50 seconds to run.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8p_LT8Sp0Lqo","colab_type":"code","outputId":"cff3e28b-290e-4bc7-b8d0-84bb2b7063c0","executionInfo":{"status":"ok","timestamp":1591167087732,"user_tz":240,"elapsed":1395,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# Pre-processing a document.\n","\n","from nltk import word_tokenize\n","download('punkt')  # Download data for tokenizer.\n","\n","def preprocess(doc):\n","    doc = doc.lower()  # Lower the text.\n","    doc = word_tokenize(doc)  # Split into words.\n","    doc = [w for w in doc if not w in stop_words]  # Remove stopwords.\n","    doc = [w for w in doc if w.isalpha()]  # Remove numbers and punctuation.\n","    return doc"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2ypQFYiAt0FG","colab_type":"code","colab":{}},"source":["#Create pandas dataframe on dataset\n","import pandas as pd\n","#https://data.world/crowdflower/ecommerce-search-relevance\n","df = pd.read_csv('https://query.data.world/s/iccclhgzbyedtv54p5lresglnun3qz', encoding = \"ISO-8859-1\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LTyM-vt6t0FI","colab_type":"code","colab":{}},"source":["#create source document and target document we will be comparing scores for\n","numDataPoints = 6000\n","\n","sourceTitle = df['product_title'][2584]\n","source_doc = df['product_description'][2584]\n","\n","targetTitles = df['product_title'][:numDataPoints].tolist()\n","target_docs = df['product_description'][:numDataPoints].tolist()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7hZ0bzdUYpsG","colab_type":"code","colab":{}},"source":["#using WmdSimilarity\n","\n","wmd_corpus = []\n","documents = []\n","documentTitles = []\n","\n","for i in range(len(target_docs)):\n","    if target_docs[i] == target_docs[i]: #this is to check if there is an empty string\n","      # ipdb.set_trace()\n","      target_docs_preproc = preprocess(target_docs[i])\n","      if target_docs_preproc != []:\n","        wmd_corpus.append(target_docs_preproc)\n","      else:\n","        wmd_corpus.append([])\n","    else:\n","      wmd_corpus.append([])\n","    documentTitles.append(targetTitles[i])\n","    documents.append(target_docs[i])\n","# Initialize WmdSimilarity.\n","from gensim.similarities import WmdSimilarity\n","num_best = 10\n","instance = WmdSimilarity(wmd_corpus, model, num_best=10)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QnL_IkLlbP6f","colab_type":"code","colab":{}},"source":["start = time()\n","\n","query = preprocess(source_doc)\n","\n","sims = instance[query]  # A query is simply a \"look-up\" in the similarity class."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hZkjyOXfboCV","colab_type":"code","outputId":"0cf2c1a7-79cb-4c2f-d0d0-5e6ea1b01e04","executionInfo":{"status":"ok","timestamp":1591167254133,"user_tz":240,"elapsed":685,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"colab":{"base_uri":"https://localhost:8080/","height":561}},"source":["# Print the query and the retrieved documents, together with their similarities.\n","print('Query:')\n","print(sourceTitle)\n","for i in range(num_best):\n","    print()\n","    print('sim = %.4f' % sims[i][1])\n","    print(documentTitles[sims[i][0]])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Query:\n","Go Green AAA Alkaline Battery 8-pack\n","\n","sim = 1.0000\n","Go Green AAA Alkaline Battery 8-pack\n","\n","sim = 0.4990\n","Duracell Procell Alkaline AAA Batteries (Case of 24)\n","\n","sim = 0.4986\n","Duracell Coppertop Alkaline AAA Batteries (Case of 24)\n","\n","sim = 0.4970\n","Energizer Industrial Alkaline AAA Batteries (Case of 24)\n","\n","sim = 0.4948\n","SmartyKat SweetGreens Cat Grass Kit\n","\n","sim = 0.4865\n","Innovera Alkaline AA Batteries (Pack of 8)\n","\n","sim = 0.4859\n","Rayovac Ultra Pro Alkaline AA Batteries (Case of 48)\n","\n","sim = 0.4853\n","Duracell Procell Alkaline AA Batteries (Case of 24)\n","\n","sim = 0.4840\n","Medline AAA Alkaline Battery (Case of 144)\n","\n","sim = 0.4840\n","Medline MedCell Alkaline Batteries, AA (Case of 144)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yXrFiyvuMIBu","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}